{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsF8yRafNKjw"
      },
      "source": [
        "This notebook trains a binary classifier on a dataset which contains movie reviews which are labelled as containing either *positive* or *negative* sentiment towards the movie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzUZxeMbRPoM"
      },
      "source": [
        "First we will install *sklearn* which we will be using to do the machine learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV8dcUsoOA_l",
        "outputId": "29f63dc9-908d-4cc1-9f83-a04cd179bf34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn) (1.23.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn) (1.9.3)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn) (3.1.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekmP1Ry1R00y"
      },
      "source": [
        "Next we will install the dataset. We will use the IMDB sentiment analysis dataset available from the [huggingface datasets library](https://huggingface.co/datasets/imdb) and described in [Maas et al. 2011](https://aclanthology.org/P11-1015.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yd0bLG6nOE4D",
        "outputId": "8a0cff2a-7c8c-4413-fac9-9b7cc7f6804d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (2.14.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from datasets) (1.23.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from datasets) (13.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from datasets) (1.5.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<2023.9.0,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fsspec[http]<2023.9.0,>=2023.1.0->datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from datasets) (0.17.2)\n",
            "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas->datasets) (2022.5)\n",
            "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h6B6dXHSP6X"
      },
      "source": [
        "Now let's load the IMDB training set. We will print out the last instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOO5rQFHUg8D",
        "outputId": "19171d0b-bb6a-49d4-b240-a249c5cd664a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': 'The story centers around Barry McKenzie who must go to England if he wishes to claim his inheritance. Being about the grossest Aussie shearer ever to set foot outside this great Nation of ours there is something of a culture clash and much fun and games ensue. The songs of Barry McKenzie(Barry Crocker) are highlights.', 'label': 1}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "imdb_dataset = load_dataset(\"imdb\")['train']\n",
        "print(imdb_dataset[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enwDYpN7Hwgw"
      },
      "source": [
        "Let's convert the training data into the format expected by scikit-learn - a list of input vectors (documents) and a list of associated output labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8xfDeEMWq1o",
        "outputId": "9b9b97c8-a9d7-4cd3-c4d5-ca4a08c11d53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The story centers around Barry McKenzie who must go to England if he wishes to claim his inheritance. Being about the grossest Aussie shearer ever to set foot outside this great Nation of ours there is something of a culture clash and much fun and games ensue. The songs of Barry McKenzie(Barry Crocker) are highlights.\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "train_data = []\n",
        "train_data_labels = []\n",
        "for item in imdb_dataset:\n",
        "  train_data.append(item['text'])\n",
        "  train_data_labels.append(item['label'])\n",
        "print(train_data[-1])\n",
        "print(train_data_labels[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI6ab7wOIOu2"
      },
      "source": [
        "We'll use the CountVectorizer class to extract the words in each review as the features the algorithm will learn from. Each document is represented as a 1000 dimension vector of word  counts. Counts > 1 are clipped to 1. Stop words are removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vDYo_rZkXZUZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def custom_preprocessor(text):\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Join tokens back into a sentence\n",
        "    processed_text = ' '.join(tokens)\n",
        "    return processed_text\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    analyzer='word',\n",
        "    max_features=16000,\n",
        "    lowercase=True,\n",
        "    binary=True,\n",
        "    ngram_range=(1, 2),\n",
        "    stop_words=['or', 'if', 'from', 'so', 'film', 'movie', 'movies', 'films','plot','was','make','sense','time','same','CGI','sense','ever','minute','hard','hired','managed','tears','fell','must','you','matter','forget','experience'],\n",
        "    preprocessor=custom_preprocessor\n",
        ")\n",
        "\n",
        "features = vectorizer.fit_transform(train_data).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7l9Xg1TTfkV"
      },
      "source": [
        "As a sanity check, let's check we have a 2-d array where each row is one of the 25,000 instances and each column is one of 16000 words or word bigrams. Print out the ngrams that will be used for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oW0zYH0TdPm",
        "outputId": "c063fa4b-8cd5-490e-e0f9-7eb34375ade9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(25000, 16000)\n",
            "['ABC' 'ALL' 'AND' ... 'zero' 'zombie' 'zombies']\n"
          ]
        }
      ],
      "source": [
        "print(features.shape)\n",
        "print(vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_vqmxuNJKXs"
      },
      "source": [
        "Split the data into a training and validation (dev) set. We'll use the validation set to test our model. We'll use 75% of the data for training and 25% for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IgFqymeXcGzG"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Assuming 'features' is your feature matrix and 'labels' is your target variable\n",
        "# Select the top k features based on chi-squared (chi2) test\n",
        "k_best = SelectKBest(score_func=chi2, k=16000)  # Adjust 'k' as needed\n",
        "selected_features = k_best.fit_transform(features, train_data_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive-Bayes Accuracy: 0.8752\n",
            "[[2188  327]\n",
            " [ 297 2188]]\n",
            "\n",
            "Logistic Regression Accuracy: 0.8918\n",
            "[[2240  275]\n",
            " [ 266 2219]]\n",
            "\n",
            "Random Forest Accuracy: 0.835\n",
            "[[2089  426]\n",
            " [ 399 2086]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(selected_features, train_data_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train your classifier\n",
        "model_nb = MultinomialNB()\n",
        "model_lr = LogisticRegression()\n",
        "model_rf = RandomForestClassifier()\n",
        "model_nb = model_nb.fit(X=X_train,y=y_train)\n",
        "model_lr = model_lr.fit(X=X_train,y=y_train)\n",
        "model_rf = model_rf.fit(X=X_train,y=y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "prediction_nb = model_nb.predict(X_test)\n",
        "prediction_lr = model_lr.predict(X_test)\n",
        "prediction_rf = model_rf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_nb = accuracy_score(y_test, prediction_nb)\n",
        "accuracy_lr = accuracy_score(y_test, prediction_lr)\n",
        "accuracy_rf = accuracy_score(y_test, prediction_rf)\n",
        "print(\"Naive-Bayes Accuracy:\", accuracy_nb)\n",
        "print(confusion_matrix(y_test,prediction_nb))\n",
        "print()\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_lr)\n",
        "print(confusion_matrix(y_test,prediction_lr))\n",
        "print()\n",
        "print(\"Random Forest Accuracy:\", accuracy_rf)\n",
        "print(confusion_matrix(y_test,prediction_rf))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQTvGAtwJWn3"
      },
      "source": [
        "Test the model on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4QXnn-92gQVD"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "models = {\n",
        "    'model_nb': model_nb, \n",
        "    'model_lr': model_lr,  \n",
        "    'model_rf': model_rf,\n",
        "}\n",
        "for model_name, model in models.items():\n",
        "    pickle.dump(model, open(f'ca337-{model_name}-model.pkl', \"wb\"))\n",
        "    \n",
        "pickle.dump(vectorizer,open('ca337-nb1000-features.pkl','wb'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
